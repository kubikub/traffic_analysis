{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m17:25:06\u001b[0m :: \u001b[1;35m   Helper    \u001b[0m :: \u001b[1;31m\u001b[2;33mWARNING \u001b[0m :: \u001b[1;37mGStreamer not found!\u001b[0m\n",
      "\u001b[32m17:25:06\u001b[0m :: \u001b[1;35m   Helper    \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mSelecting `best` resolution for streams.\u001b[0m\n",
      "\u001b[32m17:25:06\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;36m  INFO  \u001b[0m :: \u001b[1;37mVerifying Streaming URL using yt-dlp backend. Please wait...\u001b[0m\n",
      "\u001b[32m17:25:07\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;36m  INFO  \u001b[0m :: \u001b[1;37m[Backend] :: Streaming URL is fully supported. Available Streams are: [144p, 240p, 360p, 480p, 720p, best, worst]\u001b[0m\n",
      "\u001b[32m17:25:07\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;31m\u001b[2;33mWARNING \u001b[0m :: \u001b[1;37mLivestream URL detected. It is advised to use GStreamer backend(`cv2.CAP_GSTREAMER`) with it.\u001b[0m\n",
      "\u001b[32m17:25:07\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mUsing `best` resolution for streaming.\u001b[0m\n",
      "\u001b[32m17:25:07\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mYouTube source ID: `z545k7Tcb5o`, Title: `Périphérique Nord - Porte de la Pape 2024-03-07 17:25`, Quality: `best`\u001b[0m\n",
      "\u001b[32m17:25:07\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mEnabling Threaded Queue Mode for the current video source!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'title', 'formats', 'thumbnails', 'thumbnail', 'description', 'channel_id', 'channel_url', 'view_count', 'average_rating', 'age_limit', 'webpage_url', 'categories', 'tags', 'playable_in_embed', 'live_status', 'release_timestamp', '_format_sort_fields', 'automatic_captions', 'subtitles', 'comment_count', 'chapters', 'heatmap', 'concurrent_view_count', 'channel', 'channel_follower_count', 'uploader', 'uploader_id', 'uploader_url', 'upload_date', 'availability', 'original_url', 'webpage_url_basename', 'webpage_url_domain', 'extractor', 'extractor_key', 'playlist', 'playlist_index', 'display_id', 'fulltitle', 'release_date', 'is_live', 'was_live', 'requested_subtitles', '_has_drm', 'epoch', 'format_id', 'format_index', 'url', 'manifest_url', 'tbr', 'ext', 'fps', 'protocol', 'preference', 'quality', 'has_drm', 'width', 'height', 'vcodec', 'acodec', 'dynamic_range', 'source_preference', 'resolution', 'aspect_ratio', 'http_headers', 'video_ext', 'audio_ext', 'vbr', 'abr', 'format'])\n",
      "30.0\n",
      "95 - 1280x720\n",
      "None\n",
      "audio only\n",
      "audio only\n",
      "256x144\n",
      "256x144\n",
      "426x240\n",
      "426x240\n",
      "640x360\n",
      "640x360\n",
      "854x480\n",
      "854x480\n",
      "1280x720\n",
      "1280x720\n",
      "https://manifest.googlevideo.com/api/manifest/hls_playlist/expire/1709850307/ei/Y-rpZaezDeujxN8PwcOI0Ao/ip/2a01:e0a:98b:f390:4672:df19:8398:6869/id/z545k7Tcb5o.1/itag/232/source/yt_live_broadcast/requiressl/yes/ratebypass/yes/live/1/sgovp/gir%3Dyes%3Bitag%3D136/rqh/1/hls_chunk_host/rr3---sn-25ge7nzs.googlevideo.com/xpc/EgVo2aDSNQ%3D%3D/playlist_duration/3600/manifest_duration/3600/vprv/1/playlist_type/DVR/initcwndbps/1657500/mh/SJ/mm/44/mn/sn-25ge7nzs/ms/lva/mv/m/mvi/3/pl/52/dover/13/pacing/0/short_key/1/keepalive/yes/fexp/24007246/mt/1709828447/sparams/expire,ei,ip,id,itag,source,requiressl,ratebypass,live,sgovp,rqh,xpc,playlist_duration,manifest_duration,vprv,playlist_type/sig/AJfQdSswRQIhAJWrBjhU0lnEWz5vls4fs1O9alRH2Y7QCnPjZf4UdbbmAiANbpQjph9_V75ylyS-L_quFutL4bO2Ke-xgxQ-zy9hUA%3D%3D/lsparams/hls_chunk_host,initcwndbps,mh,mm,mn,ms,mv,mvi,pl/lsig/APTiJQcwRAIgfmirYqS7mVwEPGSvlxNoKvNdQt1tnVQYByjfTHsyImcCIDi4DpHURHSkqOkMqe_q9jvauJo2EHco6M4IwClLpkT6/playlist/index.m3u8\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "from vidgear.gears import CamGear\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "source=\"https://youtu.be/z545k7Tcb5o\"\n",
    "# Add YouTube Video URL as input source (for e.g https://youtu.be/bvetuLwJIkA)\n",
    "# and enable Stream Mode (`stream_mode = True`)\n",
    "stream = CamGear(\n",
    "    source=source, stream_mode=True, logging=True,  time_delay=0\n",
    ").start()\n",
    "video_metadata=stream.ytv_metadata\n",
    "\n",
    "print(video_metadata.keys())\n",
    "\n",
    "print(video_metadata['fps'])\n",
    "print(video_metadata['format'])\n",
    "print(video_metadata['format_index'])\n",
    "\n",
    "# search available resolution\n",
    "resolutions=[format['resolution'] for format in video_metadata['formats']]\n",
    "for res in resolutions:\n",
    "    print(res)\n",
    "\n",
    "# select the desired resolution to get right url \n",
    "desired_resolution = '1280x720'\n",
    "for format in video_metadata['formats']:\n",
    "    \n",
    "    if format['resolution'] == desired_resolution:\n",
    "        VIDEO = format['url']\n",
    "        break\n",
    "\n",
    "print(VIDEO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8s.pt to 'yolov8s.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21.5M/21.5M [00:02<00:00, 7.63MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "VideoInfo(width=1280, height=720, fps=30, total_frames=-3074457345618259)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict, deque\n",
    "# os.environ.pop(\"QT_QPA_PLATFORM_PLUGIN_PATH\")\n",
    "# from supervision import draw_text , Color\n",
    "from ultralytics import YOLO\n",
    "# load yolo model and get class name\n",
    "MODEL = \"yolov8s.pt\"\n",
    "model=YOLO(MODEL)\n",
    "CLASS_NAMES_DICT = model.model.names\n",
    "print(CLASS_NAMES_DICT)\n",
    "# load openvino model to get faster FPS \n",
    "model = YOLO(\"/media/storage/OneDrive-4iTEC/Documents/GitHub/My_notebooks/traffic_analysis/yolov8s_openvino_model/\", task='detect')\n",
    "# model=YOLO(MODEL)\n",
    "# model.fuse()\n",
    "\n",
    "colors = sv.ColorPalette.LEGACY\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(VIDEO)\n",
    "print(video_info)\n",
    "# calculate ratio between video stream and displayed size (here's 1280)\n",
    "\n",
    "coef=video_info.width/1280\n",
    "# print(coef)\n",
    "\n",
    "# polygon design \n",
    "#  ----> x\n",
    "# |         (x4,y4)   (x3,y3)\n",
    "# |              +-------+\n",
    "#               +-------+\n",
    "# y            +-------+\n",
    "#         (x1,y1)    (x2,y2)\n",
    "\n",
    "# 3 polygons so 3 values in each coordinate from left to right \n",
    "#    [zone1,zone2, zone3]\n",
    "x1 = [-160 , -25 , 971  ] \n",
    "y1 = [ 405 , 710 , 671  ]\n",
    "x2 = [ 112 , 568 , 1480 ]\n",
    "y2 = [ 503 , 710 , 671  ]\n",
    "x3 = [ 557 , 706 , 874  ]\n",
    "y3 = [ 195 , 212 , 212  ]\n",
    "x4 = [ 411 , 569 , 749  ]\n",
    "y4 = [ 195 , 212 , 212  ]\n",
    "# transform according video stream and displayed video ratio \n",
    "x1, y1, x2, y2, x3, y3, x4, y4 = map(lambda x: [value * coef for value in x], [x1, y1, x2, y2, x3, y3, x4, y4])\n",
    "\n",
    "\n",
    "# search middle point of the polygon (x1+x4)/2) or tier point from top ( x1 + 2* x4) / 3) to draw line for counting \n",
    "x14 = [( x1 + 2 * x4) / 3\n",
    "       for x1,x4\n",
    "       in zip(x1,x4)]\n",
    "y14 = [( y1 + 2 * y4) / 3\n",
    "       for y1,y4\n",
    "       in zip(y1,y4)]\n",
    "x23 = [ ( x2 + 2 * x3) / 3\n",
    "       for x2,x3\n",
    "       in zip(x2,x3)]\n",
    "y23 = [( y2 + 2 * y3) / 3\n",
    "       for y2,y3\n",
    "       in zip(y2,y3)]\n",
    "\n",
    "# polygon zone from left to right (becarefull must be in the same order than le linezone)\n",
    "polygons = [\n",
    "  np.array([\n",
    " [x1, y1],[x2 , y2],[x3 , y3],[x4 , y4]\n",
    "  ],np.int32)\n",
    " for x1,y1,x2,y2,x3,y3,x4,y4\n",
    " in zip(x1,y1,x2,y2,x3,y3,x4,y4)\n",
    "]\n",
    "\n",
    "\n",
    "# initialize our zones\n",
    "\n",
    "zones = [\n",
    "    sv.PolygonZone(\n",
    "        polygon=polygon,\n",
    "        frame_resolution_wh=video_info.resolution_wh\n",
    "    )\n",
    "    for polygon\n",
    "    in polygons\n",
    "]\n",
    "zone_annotators = [\n",
    "    sv.PolygonZoneAnnotator(\n",
    "        zone=zone,\n",
    "        color=colors.by_idx(index),\n",
    "        thickness=2,\n",
    "        text_thickness=1,\n",
    "        text_scale=0.5,\n",
    "    )\n",
    "    for index, zone\n",
    "    in enumerate(zones)\n",
    "]\n",
    "\n",
    "label_annotators=[\n",
    "    sv.LabelAnnotator(\n",
    "        text_position=sv.Position.TOP_CENTER,\n",
    "        color=colors.by_idx(index),\n",
    "        text_thickness=1,\n",
    "        text_scale=0.5,\n",
    "        )\n",
    "        for index \n",
    "        in range(len(zones))\n",
    "]\n",
    "\n",
    "# box_annotators = [\n",
    "#     sv.BoxAnnotator(\n",
    "#         color=colors.by_idx(index),\n",
    "#         thickness=1,\n",
    "#         text_thickness=1,\n",
    "#         text_scale=0.5\n",
    "#         )\n",
    "#     for index\n",
    "#     in range(len(polygons))\n",
    "# ]\n",
    "box_annotators = [\n",
    "    sv.BoundingBoxAnnotator(\n",
    "        color=colors.by_idx(index),\n",
    "        thickness=1,\n",
    "        )\n",
    "    for index\n",
    "    in range(len(polygons))\n",
    "]\n",
    "\n",
    "trace_annotators=[\n",
    "    sv.TraceAnnotator(\n",
    "        color=colors.by_idx(index),\n",
    "        thickness=1,\n",
    "        trace_length=video_info.fps * 1.5,\n",
    "        position=sv.Position.BOTTOM_CENTER,\n",
    "        )\n",
    "    for index\n",
    "    in range(len(polygons))\n",
    "]\n",
    "\n",
    "\n",
    "lines_start=[\n",
    "   \n",
    "    sv.Point(x14, y14)\n",
    "    for x14,y14\n",
    "    in zip(x14,y14)\n",
    " \n",
    "]\n",
    "\n",
    "lines_end =[\n",
    "    \n",
    "    sv.Point(x23, y23)\n",
    "    for x23,y23\n",
    "    in zip(x23,y23)\n",
    "]\n",
    "\n",
    "positions=[(sv.Position.CENTER,sv.Position.CENTER),\n",
    "           (sv.Position.CENTER,sv.Position.CENTER),\n",
    "           (sv.Position.CENTER,sv.Position.CENTER),\n",
    "          ]\n",
    "\n",
    "line_zones=[ sv.LineZone(start=line_start, end=line_end, triggering_anchors=position)\n",
    "            for line_start, line_end, position\n",
    "            in zip(lines_start,lines_end,positions)\n",
    "]\n",
    "\n",
    "# for automatic line zone annotator not use here want to use a custom one\n",
    "line_zone_annotators=[sv.LineZoneAnnotator(thickness=1,\n",
    "                                           color=colors.by_idx(index),\n",
    "                                            text_thickness=1,\n",
    "                                              text_scale=0.5,\n",
    "                                                text_offset=4)\n",
    "    for index\n",
    "    in range(len(line_zones))\n",
    "]\n",
    "\n",
    "# couting line zone text position \n",
    "text_pos=[ sv.Point (x=100,y=320),\n",
    "            sv.Point (x=700,y=320),\n",
    "            sv.Point (x=1077,y=320)\n",
    "\n",
    "]\n",
    "byte_tracker = sv.ByteTrack(track_thresh=0.25, track_buffer=100, match_thresh=0.8, frame_rate=video_info.fps)\n",
    "\n",
    "# byte_tracker = sv.ByteTrack()\n",
    "fps_monitor=sv.FPSMonitor()\n",
    "heat_map = sv.HeatMapAnnotator ()\n",
    "smoother = sv.DetectionsSmoother()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES = np.array([[\n",
    "    [x4[0], y4[0]], \n",
    "    [x3[0], y3[0]], \n",
    "    [x2[0], y2[0]], \n",
    "    [x1[0], y1[0]]\n",
    "\n",
    "],[ [x4[1], y4[1]], \n",
    "    [x3[1], y3[1]], \n",
    "    [x2[1], y2[1]], \n",
    "    [x1[1], y1[1]]\n",
    "],\n",
    "\n",
    "[\n",
    "    [x4[2], y4[2]], \n",
    "    [x3[2], y3[2]], \n",
    "    [x2[2], y2[2]], \n",
    "    [x1[2], y1[2]]\n",
    "]])\n",
    "\n",
    "#zone1 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 75\n",
    "\n",
    "TARGETS = np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "])\n",
    "\n",
    "#zone 2 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 85\n",
    "\n",
    "TARGETS= np.append(TARGETS, np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "]), axis=0)\n",
    "\n",
    "#zone3 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 80\n",
    "\n",
    "\n",
    "TARGETS = np.append(TARGETS, np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "]),axis=0)\n",
    "\n",
    "TARGETS = TARGETS.reshape(3, 4, 2)\n",
    "\n",
    "\n",
    "\n",
    "class ViewTransformer:\n",
    "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
    "        source = source.astype(np.float32)\n",
    "        target = target.astype(np.float32)\n",
    "        self.m = cv2.getPerspectiveTransform(source, target)\n",
    "\n",
    "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
    "        if points.size == 0:\n",
    "            return points\n",
    "\n",
    "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
    "        transformed_points = cv2.perspectiveTransform(\n",
    "                reshaped_points, self.m)\n",
    "        return transformed_points.reshape(-1, 2)\n",
    "\n",
    "# create the transformers matrix for each zone\n",
    "view_transformers=[ViewTransformer(source=s, target=t)\n",
    "                  for s,t\n",
    "                  in zip(SOURCES, TARGETS)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "selected_classes = [2, 3, 5, 7] # car, motorcycle, bus, truck from coco classes\n",
    "# initialize the dictionary that we will use to store the coordinates for each zone\n",
    "coordinates = defaultdict(lambda: deque(maxlen=30))\n",
    "coordinates = np.append(coordinates,defaultdict(lambda: deque(maxlen=30)))\n",
    "coordinates = np.append(coordinates,defaultdict(lambda: deque(maxlen=30)))                     \n",
    "\n",
    "def process_frame(frame: np.ndarray, fps) -> np.ndarray:\n",
    "    speed_labels = [],[],[] \n",
    "       \n",
    "    results = model(frame, imgsz=640, verbose=False)[0]\n",
    "    # results = model(frame)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    detections = detections[np.isin(detections.class_id, selected_classes)] # filer on selected classes\n",
    "    detections = byte_tracker.update_with_detections(detections)\n",
    "    # detections = smoother.update_with_detections(detections)\n",
    "\n",
    "    # copy frame before annotate                      \n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    for i, (zone, zone_annotator, box_annotator, trace_annotator, line_zone, line_zone_annotator, label_annotator,line_start, line_end,view_transformer,speed_label,coordinate) in  enumerate(zip(zones, \n",
    "                                                                                                                                                       zone_annotators,\n",
    "                                                                                                                                                       box_annotators,\n",
    "                                                                                                                                                       trace_annotators,\n",
    "                                                                                                                                                       line_zones,\n",
    "                                                                                                                                                       line_zone_annotators,\n",
    "                                                                                                                                                       label_annotators,\n",
    "                                                                                                                                                       lines_start,\n",
    "                                                                                                                                                         lines_end,\n",
    "                                                                                                                                                         view_transformers,\n",
    "                                                                                                                                                         speed_labels,\n",
    "                                                                                                                                                         coordinates)):\n",
    "\n",
    "        mask = zone.trigger(detections=detections)\n",
    "        detections_filtered = detections[mask]\n",
    "      \n",
    "        points = detections_filtered.get_anchors_coordinates(\n",
    "                anchor=sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "        # plug the view transformer into an existing detection pipeline\n",
    "        \n",
    "        points = view_transformer.transform_points(points=points).astype(int)\n",
    "        \n",
    "        for tracker_id, [_, y] in zip(detections_filtered.tracker_id, points):\n",
    "            coordinate[tracker_id].append(y)\n",
    "\n",
    "        # wait to have enough data\n",
    "        for tracker_id in detections_filtered.tracker_id:\n",
    "                        if len(coordinate[tracker_id]) < fps/2:\n",
    "                            # print(coordinates[tracker_id], \" - id :\", tracker_id, 'len : ', len(coordinates[tracker_id]))\n",
    "                            speed_label.append(f\"#{tracker_id}\")\n",
    "                            \n",
    "                        else:\n",
    "                            try:\n",
    "                                coordinate_start = coordinate[tracker_id][-1]\n",
    "                                coordinate_end = coordinate[tracker_id][0]\n",
    "                                distance = abs(coordinate_start - coordinate_end)\n",
    "                                time = len(coordinate[tracker_id]) / fps\n",
    "                                speed = distance / time * 3.6\n",
    "                                speed_label.append(f\"{int(speed)} km/h\")\n",
    "\n",
    "                            except: \n",
    "\n",
    "                                speed_label.append(f\"#{tracker_id}\")\n",
    "\n",
    "                                pass\n",
    "        # labels = [\n",
    "        # f\"#{tracker_id} \"\n",
    "        # for _,_,_,_,tracker_id in detections_filtered]\n",
    "        # line_zone.trigger(detections=detections_filtered)\n",
    "        annotated_frame = sv.draw_line(scene=annotated_frame, start=line_start, end=line_end, color=colors.by_idx(i) )\n",
    "        # annotated_frame = zone_annotator.annotate(scene=annotated_frame, label=f\"Dir. Ouest : {i+random.randint(0,100)}\")\n",
    "        \n",
    "        annotated_frame = zone_annotator.annotate(scene=annotated_frame, label=f\"Dir. Ouest : {line_zone.in_count}\") if i==0 else zone_annotator.annotate(scene=annotated_frame, \n",
    "                                                                                                                                                          label=f\"Dir. Est : {line_zone.out_count}\") \n",
    "        annotated_frame = label_annotator.annotate(scene=annotated_frame,\n",
    "                                                  detections=detections_filtered,\n",
    "                                                  labels=speed_label)\n",
    "        \n",
    "        # annotated_frame=line_zone_annotator.annotate(annotated_frame,line_counter=line_zone )\n",
    "        annotated_frame = box_annotator.annotate(scene=annotated_frame,\n",
    "                                                  detections=detections_filtered,\n",
    "                                                  )\n",
    "        \n",
    "        annotated_frame = trace_annotator.annotate(scene=annotated_frame,detections=detections_filtered )\n",
    "        line_zone.trigger(detections=detections_filtered)\n",
    "        # print(line_zone.in_count)\n",
    "        # print(line_zone.out_count)\n",
    "       \n",
    "\n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 30\n",
      "image : 1280x720\n",
      "Loading /media/storage/OneDrive-4iTEC/Documents/GitHub/My_notebooks/traffic_analysis/yolov8s_openvino_model for OpenVINO inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/media/storage/python_envs/supervision/lib/python3.11/site-packages/cv2/qt/plugins\"\n"
     ]
    }
   ],
   "source": [
    "# for direct show\n",
    "cap = cv2.VideoCapture(VIDEO)  \n",
    "# fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fps=video_info.fps\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"image : {width}x{height}\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # frame=cv2.resize(frame,(1280,720))\n",
    "    show=process_frame(frame,int(fps))\n",
    "   \n",
    "    fps_monitor.tick()\n",
    "    fps = fps_monitor()\n",
    "    fps_text = f\"FPS: {fps:.0f}\"\n",
    "    cv2.putText(show, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Counting\", show)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 30\n",
      "image : 1280x720\n"
     ]
    }
   ],
   "source": [
    "# to save video instead of displaying it\n",
    "side=0\n",
    "output_file = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec vidéo pour le format MP4\n",
    "\n",
    "\n",
    "out = cv2.VideoWriter(output_file, fourcc, video_info.fps, (video_info.width, video_info.height))\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO)  \n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"image : {width}x{height}\")\n",
    "# Temps de début de l'enregistrement\n",
    "start_time = time.time()\n",
    "# Durée de l'enregistrement en secondes\n",
    "duration = 120\n",
    "\n",
    "while (time.time() - start_time) < duration:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # frame=cv2.resize(frame,(1280,720))\n",
    "    show=process_frame(frame,0,fps)\n",
    "    fps_monitor.tick()\n",
    "    fps = fps_monitor()\n",
    "    fps_text = f\"FPS: {fps:.0f}\"\n",
    "    cv2.putText(show, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # cv2.imshow(\"Counting\", show)\n",
    "    out.write(show)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.201 🚀 Python-3.11.6 torch-2.1.0+cu121 CPU (12th Gen Intel Core(TM) i5-1235U)\n",
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8m.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (49.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.2s, saved as 'yolov8m.onnx' (99.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2023.1.0-12185-9e6b00e51cd-releases/2023/1...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success ✅ 1.6s, saved as 'yolov8m_openvino_model/' (99.1 MB)\n",
      "\n",
      "Export complete (5.9s)\n",
      "Results saved to \u001b[1m/media/storage/OneDrive-4iTEC/Documents/GitHub/My_notebooks/traffic_analysis\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8m_openvino_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov8m_openvino_model imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov8m_openvino_model'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# model=YOLO(\"yolov8m.pt\")\n",
    "# model=YOLO(\"traffic_analysis.pt\")\n",
    "model=YOLO(\"yolov8m.pt\")\n",
    "model.export(format='openvino')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
