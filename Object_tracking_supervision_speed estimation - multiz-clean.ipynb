{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yt_dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "# env supervision\n",
    "# supervision 0.23.0\n",
    "# ultralytics 8.3.1\n",
    "\n",
    "from vidgear.gears import CamGear\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "source=\"https://youtu.be/z545k7Tcb5o\"\n",
    "# source=\"https://shiftup.sharepoint.com/:v:/r/sites/E-SWKApplication/Shared%20Documents/E-SWK/SX-APP/projects/SX_MON_HC2_HC2-F24G_2024-12-12_10-48-18_P656700/SX_MON_HC2_HC2-F24G_REF.mp4?csf=1&web=1&e=hBZCuP\"\n",
    "# source = \"https://shiftup.sharepoint.com/:v:/r/sites/E-SWKApplication/Shared%20Documents/E-SWK/SX-APP/projects/SX_MON_HC2_HC2-F24G_2024-12-12_10-48-18_P656700/SX_MON_HC2_HC2-F24G_REF.mp4?csf=1&web=1&e=HXhlJg&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D\"\n",
    "# Add YouTube Video URL as input source (for e.g https://youtu.be/bvetuLwJIkA)\n",
    "# and enable Stream Mode (`stream_mode = True`)\n",
    "stream = CamGear(\n",
    "    source=source, stream_mode=True, logging=True,  time_delay=0\n",
    ").start()\n",
    "video_metadata=stream.ytv_metadata\n",
    "\n",
    "print(video_metadata.keys())\n",
    "\n",
    "print(video_metadata['fps'])\n",
    "print(video_metadata['format'])\n",
    "print(video_metadata['format_index'])\n",
    "\n",
    "# search available resolution\n",
    "resolutions=[format['resolution'] for format in video_metadata['formats']]\n",
    "for res in resolutions:\n",
    "    print(res)\n",
    "\n",
    "# select the desired resolution to get right url \n",
    "desired_resolution = '1280x720'\n",
    "for format in video_metadata['formats']:\n",
    "    \n",
    "    if format['resolution'] == desired_resolution:\n",
    "        VIDEO = format['url']\n",
    "        break\n",
    "\n",
    "print(VIDEO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "# URL de la vidéo sur SharePoint\n",
    "url = 'URL_DE_VOTRE_VIDEO'\n",
    "url = \"https://shiftup.sharepoint.com/:v:/r/sites/E-SWKApplication/Shared%20Documents/E-SWK/SX-APP/projects/SX_MON_HC2_HC2-F24G_2024-12-12_10-48-18_P656700/SX_MON_HC2_HC2-F24G_REF.mp4?csf=1&web=1&e=HXhlJg&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D\"\n",
    "url = \"https://shiftup.sharepoint.com/:v:/r/sites/E-SWKApplication/Shared%20Documents/E-SWK/SX-APP/projects/SX_MON_HC2_HC2-F24G_2024-12-12_10-48-18_P656700/SX_MON_HC2_HC2-F24G_REF.mp4?csf=1&web=1&e=b0AiDH\"\n",
    "# Créer un processus FFmpeg pour lire le flux vidéo\n",
    "process = (\n",
    "    ffmpeg\n",
    "    .input(url)\n",
    "    .output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
    "    .run_async(pipe_stdout=True)\n",
    ")\n",
    "\n",
    "# Lire et afficher les frames\n",
    "while True:\n",
    "    in_bytes = process.stdout.read(640 * 480 * 3)  # Assurez-vous de connaître la taille de la vidéo\n",
    "    if not in_bytes:\n",
    "        break\n",
    "    frame = (\n",
    "        np\n",
    "        .frombuffer(in_bytes, np.uint8)\n",
    "        .reshape([480, 640, 3])\n",
    "    )\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "process.stdout.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "from supervision.metrics import F1Score\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict, deque\n",
    "# os.environ.pop(\"QT_QPA_PLATFORM_PLUGIN_PATH\")\n",
    "# from supervision import draw_text , Color\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# load yolo model and get class name\n",
    "MODEL = \"models/yolo11s.pt\"\n",
    "# MODEL = \"models/\"\n",
    "# MODEL = \"models/yolov10s.pt\"\n",
    "# MODEL = \"models/yolov9c.pt\"\n",
    "model=YOLO(MODEL)\n",
    "CLASS_NAMES_DICT = model.model.names\n",
    "print(CLASS_NAMES_DICT)\n",
    "# load openvino model to get faster FPS \n",
    "# model = YOLO(\"models/yolov8s_openvino_model/\", task='detect')\n",
    "# model = YOLO(\"models/yolov9c_openvino_model/\", task='detect')\n",
    "# model = YOLO(\"models/yolov10s_openvino_model/\", task='detect')\n",
    "model = YOLO(\"models/yolo11s_openvino_model\", task='detect')\n",
    "# model=YOLO(MODEL)\n",
    "# model.fuse()\n",
    "\n",
    "colors = sv.ColorPalette.LEGACY\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(VIDEO)\n",
    "print(video_info)\n",
    "# calculate ratio between video stream and displayed size (here's 1280)\n",
    "\n",
    "coef=video_info.width/1280\n",
    "# print(coef)\n",
    "\n",
    "# polygon design \n",
    "#  ----> x\n",
    "# |         (x4,y4)   (x3,y3)\n",
    "# |              +-------+\n",
    "#               +-------+\n",
    "# y            +-------+\n",
    "#         (x1,y1)    (x2,y2)\n",
    "\n",
    "# 3 polygons so 3 values in each coordinate from left to right \n",
    "#    [zone1,zone2, zone3]\n",
    "x1 = [-160 , -25 , 971  ] \n",
    "y1 = [ 405 , 710 , 671  ]\n",
    "x2 = [ 112 , 568 , 1480 ]\n",
    "y2 = [ 503 , 710 , 671  ]\n",
    "x3 = [ 557 , 706 , 874  ]\n",
    "y3 = [ 195 , 212 , 212  ]\n",
    "x4 = [ 411 , 569 , 749  ]\n",
    "y4 = [ 195 , 212 , 212  ]\n",
    "# transform according video stream and displayed video ratio \n",
    "x1, y1, x2, y2, x3, y3, x4, y4 = map(lambda x: [value * coef for value in x], [x1, y1, x2, y2, x3, y3, x4, y4])\n",
    "\n",
    "\n",
    "# search middle point of the polygon (x1+x4)/2) or tier point from top ( x1 + 2* x4) / 3) to draw line for counting \n",
    "x14 = [( x1 + 2 * x4) / 3\n",
    "       for x1,x4\n",
    "       in zip(x1,x4)]\n",
    "y14 = [( y1 + 2 * y4) / 3\n",
    "       for y1,y4\n",
    "       in zip(y1,y4)]\n",
    "x23 = [ ( x2 + 2 * x3) / 3\n",
    "       for x2,x3\n",
    "       in zip(x2,x3)]\n",
    "y23 = [( y2 + 2 * y3) / 3\n",
    "       for y2,y3\n",
    "       in zip(y2,y3)]\n",
    "\n",
    "# polygon zone from left to right (becarefull must be in the same order than le linezone)\n",
    "polygons = [\n",
    "  np.array([\n",
    " [x1, y1],[x2 , y2],[x3 , y3],[x4 , y4]\n",
    "  ],np.int32)\n",
    " for x1, y1, x2, y2, x3, y3, x4, y4\n",
    " in zip(x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "]\n",
    "\n",
    "\n",
    "# initialize our zones\n",
    "\n",
    "zones = [\n",
    "    sv.PolygonZone(\n",
    "        polygon = polygon,\n",
    "        # frame_resolution_wh = video_info.resolution_wh\n",
    "    )\n",
    "    for polygon\n",
    "    in polygons\n",
    "]\n",
    "zone_annotators = [\n",
    "    sv.PolygonZoneAnnotator(\n",
    "        zone = zone,\n",
    "        color = colors.by_idx(index),\n",
    "        thickness = 2,\n",
    "        text_thickness = 1,\n",
    "        text_scale = 0.5,\n",
    "    )\n",
    "    for index, zone\n",
    "    in enumerate(zones)\n",
    "]\n",
    "\n",
    "label_annotators = [\n",
    "    sv.LabelAnnotator(\n",
    "        text_position = sv.Position.TOP_CENTER,\n",
    "        color=colors.by_idx(index),\n",
    "        text_thickness = 1,\n",
    "        text_scale = 0.5,\n",
    "        )\n",
    "        for index \n",
    "        in range(len(zones))\n",
    "]\n",
    "\n",
    "# box_annotators = [\n",
    "#     sv.BoxAnnotator(\n",
    "#         color=colors.by_idx(index),\n",
    "#         thickness=1,\n",
    "#         text_thickness=1,\n",
    "#         text_scale=0.5\n",
    "#         )\n",
    "#     for index\n",
    "#     in range(len(polygons))\n",
    "# ]\n",
    "box_annotators = [\n",
    "    sv.BoxAnnotator(\n",
    "        color = colors.by_idx(index),\n",
    "        thickness = 1,\n",
    "        )\n",
    "    for index\n",
    "    in range(len(polygons))\n",
    "]\n",
    "\n",
    "trace_annotators=[\n",
    "    sv.TraceAnnotator(\n",
    "        color = colors.by_idx(index),\n",
    "        thickness = 1,\n",
    "        trace_length = video_info.fps * 1.5,\n",
    "        position = sv.Position.BOTTOM_CENTER,\n",
    "        )\n",
    "    for index\n",
    "    in range(len(polygons))\n",
    "]\n",
    "\n",
    "\n",
    "lines_start=[\n",
    "   \n",
    "    sv.Point(x14, y14)\n",
    "    for x14,y14\n",
    "    in zip(x14,y14)\n",
    " \n",
    "]\n",
    "\n",
    "lines_end =[\n",
    "    \n",
    "    sv.Point(x23, y23)\n",
    "    for x23,y23\n",
    "    in zip(x23,y23)\n",
    "]\n",
    "\n",
    "positions=[(sv.Position.CENTER,sv.Position.CENTER),\n",
    "           (sv.Position.CENTER,sv.Position.CENTER),\n",
    "           (sv.Position.CENTER,sv.Position.CENTER),\n",
    "          ]\n",
    "\n",
    "line_zones = [ sv.LineZone(start=line_start, end=line_end, triggering_anchors=position)\n",
    "            for line_start, line_end, position\n",
    "            in zip(lines_start,lines_end,positions)\n",
    "]\n",
    "\n",
    "# for automatic line zone annotator not use here want to use a custom one\n",
    "line_zone_annotators = [sv.LineZoneAnnotator(thickness = 1,\n",
    "                                           color = colors.by_idx(index),\n",
    "                                            text_thickness = 1,\n",
    "                                              text_scale = 0.5,\n",
    "                                                text_offset = 4)\n",
    "    for index\n",
    "    in range(len(line_zones))\n",
    "]\n",
    "\n",
    "# couting line zone text position \n",
    "text_pos=[ sv.Point (x = 100,y = 320),\n",
    "            sv.Point (x = 700,y = 320),\n",
    "            sv.Point (x = 1077,y = 320)\n",
    "\n",
    "]\n",
    "byte_tracker = sv.ByteTrack(track_activation_threshold=0.25, lost_track_buffer=100, minimum_matching_threshold=0.8, frame_rate=video_info.fps)\n",
    "\n",
    "# byte_tracker = sv.ByteTrack()\n",
    "fps_monitor = sv.FPSMonitor()\n",
    "heat_map = sv.HeatMapAnnotator ()\n",
    "smoother = sv.DetectionsSmoother()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES = np.array([[\n",
    "    [x4[0], y4[0]], \n",
    "    [x3[0], y3[0]], \n",
    "    [x2[0], y2[0]], \n",
    "    [x1[0], y1[0]]\n",
    "\n",
    "],[ [x4[1], y4[1]], \n",
    "    [x3[1], y3[1]], \n",
    "    [x2[1], y2[1]], \n",
    "    [x1[1], y1[1]]\n",
    "],\n",
    "\n",
    "[\n",
    "    [x4[2], y4[2]], \n",
    "    [x3[2], y3[2]], \n",
    "    [x2[2], y2[2]], \n",
    "    [x1[2], y1[2]]\n",
    "]])\n",
    "\n",
    "#zone1 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 75\n",
    "\n",
    "TARGETS = np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "])\n",
    "\n",
    "#zone 2 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 85\n",
    "\n",
    "TARGETS= np.append(TARGETS, np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "]), axis=0)\n",
    "\n",
    "#zone3 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 80\n",
    "\n",
    "\n",
    "TARGETS = np.append(TARGETS, np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "]),axis=0)\n",
    "\n",
    "TARGETS = TARGETS.reshape(3, 4, 2)\n",
    "\n",
    "\n",
    "\n",
    "class ViewTransformer:\n",
    "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
    "        source = source.astype(np.float32)\n",
    "        target = target.astype(np.float32)\n",
    "        self.m = cv2.getPerspectiveTransform(source, target)\n",
    "\n",
    "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
    "        if points.size == 0:\n",
    "            return points\n",
    "\n",
    "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
    "        transformed_points = cv2.perspectiveTransform(\n",
    "                reshaped_points, self.m)\n",
    "        return transformed_points.reshape(-1, 2)\n",
    "\n",
    "# create the transformers matrix for each zone\n",
    "view_transformers=[ViewTransformer(source=s, target=t)\n",
    "                  for s,t\n",
    "                  in zip(SOURCES, TARGETS)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "selected_classes = [2, 3, 5, 7] # car, motorcycle, bus, truck from coco classes\n",
    "# initialize the dictionary that we will use to store the coordinates for each zone\n",
    "coordinates = defaultdict(lambda: deque(maxlen=30))\n",
    "coordinates = np.append(coordinates,defaultdict(lambda: deque(maxlen=30)))\n",
    "coordinates = np.append(coordinates,defaultdict(lambda: deque(maxlen=30)))                     \n",
    "\n",
    "def process_frame(frame: np.ndarray, fps) -> np.ndarray:\n",
    "    speed_labels = [],[],[] \n",
    "       \n",
    "    results = model(frame, imgsz=640, verbose=False)[0]\n",
    "    # results = model(frame)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    detections = detections[np.isin(detections.class_id, selected_classes)] # filer on selected classes\n",
    "    detections = byte_tracker.update_with_detections(detections)\n",
    "    detections = smoother.update_with_detections(detections)\n",
    "\n",
    "    # copy frame before annotate                      \n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    for i, (zone, zone_annotator, box_annotator, trace_annotator, line_zone,\n",
    "            line_zone_annotator,\n",
    "            label_annotator,\n",
    "            line_start,\n",
    "            line_end,\n",
    "            view_transformer,\n",
    "            speed_label,coordinate\n",
    "            ) in  enumerate(zip(zones, zone_annotators, box_annotators,\n",
    "                                trace_annotators,\n",
    "                                line_zones,\n",
    "                                line_zone_annotators,\n",
    "                                label_annotators,\n",
    "                                lines_start,\n",
    "                                lines_end,\n",
    "                                view_transformers,\n",
    "                                speed_labels,\n",
    "                                coordinates\n",
    "                                )\n",
    "                            ):\n",
    "\n",
    "        mask = zone.trigger(detections=detections)\n",
    "        detections_filtered = detections[mask]\n",
    "        points = detections_filtered.get_anchors_coordinates(\n",
    "                anchor=sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "        # plug the view transformer into an existing detection pipeline\n",
    "        \n",
    "        points = view_transformer.transform_points(points=points).astype(int)\n",
    "        \n",
    "        for tracker_id, [_, y] in zip(detections_filtered.tracker_id, points):\n",
    "            coordinate[tracker_id].append(y)\n",
    "\n",
    "        # wait to have enough data\n",
    "        for tracker_id in detections_filtered.tracker_id:\n",
    "                        if len(coordinate[tracker_id]) < fps/2:\n",
    "                            # print(coordinates[tracker_id], \" - id :\", tracker_id, 'len : ', len(coordinates[tracker_id]))\n",
    "                            speed_label.append(f\"#{tracker_id}\")\n",
    "                            \n",
    "                        else:\n",
    "                            try:\n",
    "                                coordinate_start = coordinate[tracker_id][-1]\n",
    "                                coordinate_end = coordinate[tracker_id][0]\n",
    "                                distance = abs(coordinate_start - coordinate_end)\n",
    "                                time = len(coordinate[tracker_id]) / fps\n",
    "                                speed = distance / time * 3.6\n",
    "                                speed_label.append(f\"{int(speed)} km/h\")\n",
    "\n",
    "                            except: \n",
    "\n",
    "                                speed_label.append(f\"#{tracker_id}\")\n",
    "\n",
    "                                pass\n",
    "        # labels = [\n",
    "        # f\"#{tracker_id} \"\n",
    "        # for _,_,_,_,tracker_id in detections_filtered]\n",
    "        # crossed_in, crossed_out= line_zone.trigger(detections=detections_filtered)\n",
    "        # print(crossed_in, crossed_out)\n",
    "        # if line_zone.in_count > 0 or line_zone.out_count > 0:\n",
    "            # print(f\"Zone {i} : {line_zone.in_count} in, {line_zone.out_count} out\")\n",
    "        annotated_frame = sv.draw_line(scene=annotated_frame, start=line_start, end=line_end, color=colors.by_idx(i) )\n",
    "        # annotated_frame = zone_annotator.annotate(scene=annotated_frame, label=f\"Dir. Ouest : {i+random.randint(0,100)}\")\n",
    "        \n",
    "        annotated_frame = zone_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            label=f\"Dir. Ouest : {line_zone.out_count}\") if i==0 else zone_annotator.annotate(\n",
    "                scene=annotated_frame, label=f\"Dir. Est : {line_zone.in_count}\") \n",
    "            \n",
    "        annotated_frame = label_annotator.annotate(scene=annotated_frame,\n",
    "                                                  detections=detections_filtered,\n",
    "                                                  labels=speed_label)\n",
    "        \n",
    "        # annotated_frame=line_zone_annotator.annotate(annotated_frame,line_counter=line_zone )\n",
    "        annotated_frame = box_annotator.annotate(scene=annotated_frame,\n",
    "                                                  detections=detections_filtered,\n",
    "                                                  )\n",
    "        \n",
    "        annotated_frame = trace_annotator.annotate(scene=annotated_frame,detections=detections_filtered )\n",
    "        line_zone.trigger(detections=detections_filtered)\n",
    "        # print(line_zone.in_count)\n",
    "        # print(line_zone.out_count)\n",
    "       \n",
    "\n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for direct show\n",
    "cap = cv2.VideoCapture(VIDEO)  \n",
    "# fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fps=video_info.fps\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"image : {width}x{height}\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # frame=cv2.resize(frame,(1280,720))\n",
    "    show=process_frame(frame,int(fps))\n",
    "   \n",
    "    fps_monitor.tick()\n",
    "    # fps = fps_monitor()\n",
    "    fps = fps_monitor.fps\n",
    "    fps_text = f\"FPS: {fps:.0f}\"\n",
    "    cv2.putText(show, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Counting\", show)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save video instead of displaying it\n",
    "side=0\n",
    "output_file = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec vidéo pour le format MP4\n",
    "\n",
    "\n",
    "out = cv2.VideoWriter(output_file, fourcc, video_info.fps, (video_info.width, video_info.height))\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO)  \n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"image : {width}x{height}\")\n",
    "# Temps de début de l'enregistrement\n",
    "start_time = time.time()\n",
    "# Durée de l'enregistrement en secondes\n",
    "duration = 120\n",
    "\n",
    "while (time.time() - start_time) < duration:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # frame=cv2.resize(frame,(1280,720))\n",
    "    show=process_frame(frame,0,fps)\n",
    "    fps_monitor.tick()\n",
    "    # fps = fps_monitor()\n",
    "    fps = fps_monitor.fps\n",
    "    fps_text = f\"FPS: {fps:.0f}\"\n",
    "    cv2.putText(show, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # cv2.imshow(\"Counting\", show)\n",
    "    out.write(show)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# model=YOLO(\"yolov8m.pt\")\n",
    "# model=YOLO(\"traffic_analysis.pt\")\n",
    "model=YOLO(\"models/yolo11s.pt\")\n",
    "model.export(format='openvino')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
