{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yt_dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:02:49\u001b[0m :: \u001b[1;35m   Helper    \u001b[0m :: \u001b[1;36m  INFO  \u001b[0m :: \u001b[1;37mRunning VidGear Version: 0.3.3\u001b[0m\n",
      "\u001b[32m16:02:49\u001b[0m :: \u001b[1;35m   Helper    \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mSelecting `best` resolution for streams.\u001b[0m\n",
      "\u001b[32m16:02:49\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;36m  INFO  \u001b[0m :: \u001b[1;37mVerifying Streaming URL using yt-dlp backend. Please wait...\u001b[0m\n",
      "\u001b[32m16:02:52\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;36m  INFO  \u001b[0m :: \u001b[1;37m[Backend] :: Streaming URL is fully supported. Available Streams are: [144p, 240p, 360p, 480p, 720p, best, worst]\u001b[0m\n",
      "\u001b[32m16:02:52\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;31m\u001b[2;33mWARNING \u001b[0m :: \u001b[1;37mLivestream URL detected. It is strongly recommended to use the GStreamer backend (`backend=cv2.CAP_GSTREAMER`) with these URLs.\u001b[0m\n",
      "\u001b[32m16:02:52\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mUsing `best` resolution for streaming.\u001b[0m\n",
      "\u001b[32m16:02:52\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mYouTube source ID: `z545k7Tcb5o`, Title: `P√©riph√©rique Nord - Porte de la Pape 2025-03-12 16:02`, Quality: `best`\u001b[0m\n",
      "\u001b[32m16:02:52\u001b[0m :: \u001b[1;35m   CamGear   \u001b[0m :: \u001b[1;33m DEBUG  \u001b[0m :: \u001b[1;37mEnabling Threaded Queue Mode for the current video source!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'title', 'formats', 'thumbnails', 'thumbnail', 'description', 'channel_id', 'channel_url', 'view_count', 'average_rating', 'age_limit', 'webpage_url', 'categories', 'tags', 'playable_in_embed', 'live_status', 'media_type', 'release_timestamp', '_format_sort_fields', 'automatic_captions', 'subtitles', 'comment_count', 'chapters', 'heatmap', 'like_count', 'concurrent_view_count', 'channel', 'channel_follower_count', 'uploader', 'uploader_id', 'uploader_url', 'upload_date', 'timestamp', 'availability', 'original_url', 'webpage_url_basename', 'webpage_url_domain', 'extractor', 'extractor_key', 'playlist', 'playlist_index', 'display_id', 'fulltitle', 'release_date', 'release_year', 'is_live', 'was_live', 'requested_subtitles', '_has_drm', 'epoch', 'format_id', 'format_index', 'url', 'manifest_url', 'tbr', 'ext', 'fps', 'protocol', 'preference', 'quality', 'has_drm', 'width', 'height', 'vcodec', 'acodec', 'dynamic_range', 'source_preference', 'video_ext', 'audio_ext', 'vbr', 'abr', 'resolution', 'aspect_ratio', 'http_headers', 'format'])\n",
      "30.0\n",
      "95 - 1280x720\n",
      "None\n",
      "audio only\n",
      "audio only\n",
      "256x144\n",
      "256x144\n",
      "426x240\n",
      "426x240\n",
      "640x360\n",
      "640x360\n",
      "854x480\n",
      "854x480\n",
      "1280x720\n",
      "1280x720\n",
      "https://manifest.googlevideo.com/api/manifest/hls_playlist/expire/1741813371/ei/G6LRZ9b3HeL5xN8PydWCuQo/ip/92.88.163.199/id/z545k7Tcb5o.2/itag/232/source/yt_live_broadcast/requiressl/yes/ratebypass/yes/live/1/sgovp/gir%3Dyes%3Bitag%3D136/rqh/1/hls_chunk_host/rr4---sn-n4g-nmcd.googlevideo.com/xpc/EgVo2aDSNQ%3D%3D/playlist_duration/3600/manifest_duration/3600/bui/AUWDL3zEk-mLzMS6Te7hmFwaSoybgjoR0nsN5FywdtbAyrPUO74ywpI4s2ZQyBoQYaZlzf62jSuArhoU/spc/RjZbSRxE_e-9QtOkOw0LPIXxDQq6UcUDOLE3sGsk4szeX3uCQqStgcGy/vprv/1/playlist_type/DVR/met/1741791771,/mh/SJ/mm/44/mn/sn-n4g-nmcd/ms/lva/mv/u/mvi/4/pcm2cms/yes/pl/24/rms/lva,lva/dover/13/pacing/0/short_key/1/keepalive/yes/fexp/51326932,51358316,51411872/mt/1741790735/sparams/expire,ei,ip,id,itag,source,requiressl,ratebypass,live,sgovp,rqh,xpc,playlist_duration,manifest_duration,bui,spc,vprv,playlist_type/sig/AJfQdSswRAIgW2-xYGSrPIIOIHo5RvRwEwLvavc4bx7siWABrk1y_GkCIGkHJsLgDGxyMVfBYFrb6pKq2v8yamy14E5OzBd74rLz/lsparams/hls_chunk_host,met,mh,mm,mn,ms,mv,mvi,pcm2cms,pl,rms/lsig/AFVRHeAwRAIgZVhz_p3Ox3CBSmNJZVHJKkZpf8DNrDDwEtHAlEf_dSICIBfObL687vkNCk8OjL6zgaIj589rdcZhBAqfrUvFpjKf/playlist/index.m3u8\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "# env supervision\n",
    "#¬†supervision 0.23.0\n",
    "#¬†ultralytics 8.3.1\n",
    "\n",
    "from vidgear.gears import CamGear\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "source=\"https://youtu.be/z545k7Tcb5o\"\n",
    "# source=\"https://shiftup.sharepoint.com/:v:/r/sites/E-SWKApplication/Shared%20Documents/E-SWK/SX-APP/projects/SX_MON_HC2_HC2-F24G_2024-12-12_10-48-18_P656700/SX_MON_HC2_HC2-F24G_REF.mp4?csf=1&web=1&e=hBZCuP\"\n",
    "# source = \"https://shiftup.sharepoint.com/:v:/r/sites/E-SWKApplication/Shared%20Documents/E-SWK/SX-APP/projects/SX_MON_HC2_HC2-F24G_2024-12-12_10-48-18_P656700/SX_MON_HC2_HC2-F24G_REF.mp4?csf=1&web=1&e=HXhlJg&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D\"\n",
    "# Add YouTube Video URL as input source (for e.g https://youtu.be/bvetuLwJIkA)\n",
    "# and enable Stream Mode (`stream_mode = True`)\n",
    "stream = CamGear(\n",
    "    source=source, stream_mode=True, logging=True,  time_delay=0\n",
    ").start()\n",
    "video_metadata=stream.ytv_metadata\n",
    "\n",
    "print(video_metadata.keys())\n",
    "\n",
    "print(video_metadata['fps'])\n",
    "print(video_metadata['format'])\n",
    "print(video_metadata['format_index'])\n",
    "\n",
    "# search available resolution\n",
    "resolutions=[format['resolution'] for format in video_metadata['formats']]\n",
    "for res in resolutions:\n",
    "    print(res)\n",
    "\n",
    "# select the desired resolution to get right url \n",
    "desired_resolution = '1280x720'\n",
    "for format in video_metadata['formats']:\n",
    "    \n",
    "    if format['resolution'] == desired_resolution:\n",
    "        VIDEO = format['url']\n",
    "        break\n",
    "\n",
    "print(VIDEO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import ffmpeg\n",
    "import numpy as np\n",
    "# URL de la vid√©o sur SharePoint\n",
    "url = 'URL_DE_VOTRE_VIDEO'\n",
    "url = \"https://shiftup.sharepoint.com/:v:/r/sites/E-SWKApplication/Shared%20Documents/E-SWK/SX-APP/projects/SX_MON_HC2_HC2-F24G_2024-12-12_10-48-18_P656700/SX_MON_HC2_HC2-F24G_REF.mp4?csf=1&web=1&e=HXhlJg&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D\"\n",
    "url = \"https://shiftup.sharepoint.com/:v:/r/sites/E-SWKApplication/Shared%20Documents/E-SWK/SX-APP/projects/SX_MON_HC2_HC2-F24G_2024-12-12_10-48-18_P656700/SX_MON_HC2_HC2-F24G_REF.mp4?csf=1&web=1&e=b0AiDH\"\n",
    "# Cr√©er un processus FFmpeg pour lire le flux vid√©o\n",
    "process = (\n",
    "    ffmpeg\n",
    "    .input(url)\n",
    "    .output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
    "    .run_async(pipe_stdout=True)\n",
    ")\n",
    "\n",
    "# Lire et afficher les frames\n",
    "while True:\n",
    "    in_bytes = process.stdout.read(640 * 480 * 3)  # Assurez-vous de conna√Ætre la taille de la vid√©o\n",
    "    if not in_bytes:\n",
    "        break\n",
    "    frame = (\n",
    "        np\n",
    "        .frombuffer(in_bytes, np.uint8)\n",
    "        .reshape([480, 640, 3])\n",
    "    )\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "process.stdout.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "VideoInfo(width=1280, height=720, fps=30, total_frames=-3074457345618259)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "from supervision.metrics import F1Score\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from collections import defaultdict, deque\n",
    "# os.environ.pop(\"QT_QPA_PLATFORM_PLUGIN_PATH\")\n",
    "# from supervision import draw_text , Color\n",
    "from ultralytics import YOLO\n",
    "# load yolo model and get class name\n",
    "MODEL = \"models/yolo11s.pt\"\n",
    "# MODEL = \"models/\"\n",
    "# MODEL = \"models/yolov10s.pt\"\n",
    "# MODEL = \"models/yolov9c.pt\"\n",
    "model=YOLO(MODEL)\n",
    "CLASS_NAMES_DICT = model.model.names\n",
    "print(CLASS_NAMES_DICT)\n",
    "# load openvino model to get faster FPS \n",
    "# model = YOLO(\"models/yolov8s_openvino_model/\", task='detect')\n",
    "# model = YOLO(\"models/yolov9c_openvino_model/\", task='detect')\n",
    "# model = YOLO(\"models/yolov10s_openvino_model/\", task='detect')\n",
    "model = YOLO(\"models/yolo11s_openvino_model\", task='detect')\n",
    "# model=YOLO(MODEL)\n",
    "# model.fuse()\n",
    "\n",
    "colors = sv.ColorPalette.LEGACY\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(VIDEO)\n",
    "print(video_info)\n",
    "# calculate ratio between video stream and displayed size (here's 1280)\n",
    "\n",
    "coef=video_info.width/1280\n",
    "# print(coef)\n",
    "\n",
    "# polygon design \n",
    "#  ----> x\n",
    "# |         (x4,y4)   (x3,y3)\n",
    "# |              +-------+\n",
    "#               +-------+\n",
    "# y            +-------+\n",
    "#         (x1,y1)    (x2,y2)\n",
    "\n",
    "# 3 polygons so 3 values in each coordinate from left to right \n",
    "#    [zone1,zone2, zone3]\n",
    "x1 = [-160 , -25 , 971  ] \n",
    "y1 = [ 405 , 710 , 671  ]\n",
    "x2 = [ 112 , 568 , 1480 ]\n",
    "y2 = [ 503 , 710 , 671  ]\n",
    "x3 = [ 557 , 706 , 874  ]\n",
    "y3 = [ 195 , 212 , 212  ]\n",
    "x4 = [ 411 , 569 , 749  ]\n",
    "y4 = [ 195 , 212 , 212  ]\n",
    "# transform according video stream and displayed video ratio \n",
    "x1, y1, x2, y2, x3, y3, x4, y4 = map(lambda x: [value * coef for value in x], [x1, y1, x2, y2, x3, y3, x4, y4])\n",
    "\n",
    "\n",
    "# search middle point of the polygon (x1+x4)/2) or tier point from top ( x1 + 2* x4) / 3) to draw line for counting \n",
    "x14 = [( x1 + 2 * x4) / 3\n",
    "       for x1,x4\n",
    "       in zip(x1,x4)]\n",
    "y14 = [( y1 + 2 * y4) / 3\n",
    "       for y1,y4\n",
    "       in zip(y1,y4)]\n",
    "x23 = [ ( x2 + 2 * x3) / 3\n",
    "       for x2,x3\n",
    "       in zip(x2,x3)]\n",
    "y23 = [( y2 + 2 * y3) / 3\n",
    "       for y2,y3\n",
    "       in zip(y2,y3)]\n",
    "\n",
    "# polygon zone from left to right (becarefull must be in the same order than le linezone)\n",
    "polygons = [\n",
    "  np.array([\n",
    " [x1, y1],[x2 , y2],[x3 , y3],[x4 , y4]\n",
    "  ],np.int32)\n",
    " for x1, y1, x2, y2, x3, y3, x4, y4\n",
    " in zip(x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "]\n",
    "\n",
    "\n",
    "# initialize our zones\n",
    "\n",
    "zones = [\n",
    "    sv.PolygonZone(\n",
    "        polygon = polygon,\n",
    "        # frame_resolution_wh = video_info.resolution_wh\n",
    "    )\n",
    "    for polygon\n",
    "    in polygons\n",
    "]\n",
    "zone_annotators = [\n",
    "    sv.PolygonZoneAnnotator(\n",
    "        zone = zone,\n",
    "        color = colors.by_idx(index),\n",
    "        thickness = 2,\n",
    "        text_thickness = 1,\n",
    "        text_scale = 0.5,\n",
    "    )\n",
    "    for index, zone\n",
    "    in enumerate(zones)\n",
    "]\n",
    "\n",
    "label_annotators = [\n",
    "    sv.LabelAnnotator(\n",
    "        text_position = sv.Position.TOP_CENTER,\n",
    "        color=colors.by_idx(index),\n",
    "        text_thickness = 1,\n",
    "        text_scale = 0.5,\n",
    "        )\n",
    "        for index \n",
    "        in range(len(zones))\n",
    "]\n",
    "\n",
    "# box_annotators = [\n",
    "#     sv.BoxAnnotator(\n",
    "#         color=colors.by_idx(index),\n",
    "#         thickness=1,\n",
    "#         text_thickness=1,\n",
    "#         text_scale=0.5\n",
    "#         )\n",
    "#     for index\n",
    "#     in range(len(polygons))\n",
    "# ]\n",
    "box_annotators = [\n",
    "    sv.BoxAnnotator(\n",
    "        color = colors.by_idx(index),\n",
    "        thickness = 1,\n",
    "        )\n",
    "    for index\n",
    "    in range(len(polygons))\n",
    "]\n",
    "\n",
    "trace_annotators=[\n",
    "    sv.TraceAnnotator(\n",
    "        color = colors.by_idx(index),\n",
    "        thickness = 1,\n",
    "        trace_length = video_info.fps * 1.5,\n",
    "        position = sv.Position.BOTTOM_CENTER,\n",
    "        )\n",
    "    for index\n",
    "    in range(len(polygons))\n",
    "]\n",
    "\n",
    "\n",
    "lines_start=[\n",
    "   \n",
    "    sv.Point(x14, y14)\n",
    "    for x14,y14\n",
    "    in zip(x14,y14)\n",
    " \n",
    "]\n",
    "\n",
    "lines_end =[\n",
    "    \n",
    "    sv.Point(x23, y23)\n",
    "    for x23,y23\n",
    "    in zip(x23,y23)\n",
    "]\n",
    "\n",
    "positions=[(sv.Position.CENTER,sv.Position.CENTER),\n",
    "           (sv.Position.CENTER,sv.Position.CENTER),\n",
    "           (sv.Position.CENTER,sv.Position.CENTER),\n",
    "          ]\n",
    "\n",
    "line_zones = [ sv.LineZone(start=line_start, end=line_end, triggering_anchors=position)\n",
    "            for line_start, line_end, position\n",
    "            in zip(lines_start,lines_end,positions)\n",
    "]\n",
    "\n",
    "# for automatic line zone annotator not use here want to use a custom one\n",
    "line_zone_annotators = [sv.LineZoneAnnotator(thickness = 1,\n",
    "                                           color = colors.by_idx(index),\n",
    "                                            text_thickness = 1,\n",
    "                                              text_scale = 0.5,\n",
    "                                                text_offset = 4)\n",
    "    for index\n",
    "    in range(len(line_zones))\n",
    "]\n",
    "\n",
    "# couting line zone text position \n",
    "text_pos=[ sv.Point (x = 100,y = 320),\n",
    "            sv.Point (x = 700,y = 320),\n",
    "            sv.Point (x = 1077,y = 320)\n",
    "\n",
    "]\n",
    "byte_tracker = sv.ByteTrack(track_activation_threshold=0.25, lost_track_buffer=100, minimum_matching_threshold=0.8, frame_rate=video_info.fps)\n",
    "\n",
    "# byte_tracker = sv.ByteTrack()\n",
    "fps_monitor = sv.FPSMonitor()\n",
    "heat_map = sv.HeatMapAnnotator ()\n",
    "smoother = sv.DetectionsSmoother()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES = np.array([[\n",
    "    [x4[0], y4[0]], \n",
    "    [x3[0], y3[0]], \n",
    "    [x2[0], y2[0]], \n",
    "    [x1[0], y1[0]]\n",
    "\n",
    "],[ [x4[1], y4[1]], \n",
    "    [x3[1], y3[1]], \n",
    "    [x2[1], y2[1]], \n",
    "    [x1[1], y1[1]]\n",
    "],\n",
    "\n",
    "[\n",
    "    [x4[2], y4[2]], \n",
    "    [x3[2], y3[2]], \n",
    "    [x2[2], y2[2]], \n",
    "    [x1[2], y1[2]]\n",
    "]])\n",
    "\n",
    "#zone1 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 75\n",
    "\n",
    "TARGETS = np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "])\n",
    "\n",
    "#zone 2 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 85\n",
    "\n",
    "TARGETS= np.append(TARGETS, np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "]), axis=0)\n",
    "\n",
    "#zone3 in meters\n",
    "TARGET_WIDTH = 6\n",
    "TARGET_HEIGHT = 80\n",
    "\n",
    "\n",
    "TARGETS = np.append(TARGETS, np.array([\n",
    "    [0, 0],\n",
    "    [TARGET_WIDTH - 1, 0],\n",
    "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
    "    [0, TARGET_HEIGHT - 1],\n",
    "]),axis=0)\n",
    "\n",
    "TARGETS = TARGETS.reshape(3, 4, 2)\n",
    "\n",
    "\n",
    "\n",
    "class ViewTransformer:\n",
    "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
    "        source = source.astype(np.float32)\n",
    "        target = target.astype(np.float32)\n",
    "        self.m = cv2.getPerspectiveTransform(source, target)\n",
    "\n",
    "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
    "        if points.size == 0:\n",
    "            return points\n",
    "\n",
    "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
    "        transformed_points = cv2.perspectiveTransform(\n",
    "                reshaped_points, self.m)\n",
    "        return transformed_points.reshape(-1, 2)\n",
    "\n",
    "# create the transformers matrix for each zone\n",
    "view_transformers=[ViewTransformer(source=s, target=t)\n",
    "                  for s,t\n",
    "                  in zip(SOURCES, TARGETS)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "selected_classes = [2, 3, 5, 7] # car, motorcycle, bus, truck from coco classes\n",
    "# initialize the dictionary that we will use to store the coordinates for each zone\n",
    "coordinates = defaultdict(lambda: deque(maxlen=30))\n",
    "coordinates = np.append(coordinates,defaultdict(lambda: deque(maxlen=30)))\n",
    "coordinates = np.append(coordinates,defaultdict(lambda: deque(maxlen=30)))                     \n",
    "\n",
    "def process_frame(frame: np.ndarray, fps) -> np.ndarray:\n",
    "    speed_labels = [],[],[] \n",
    "       \n",
    "    results = model(frame, imgsz=640, verbose=False)[0]\n",
    "    # results = model(frame)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    detections = detections[np.isin(detections.class_id, selected_classes)] # filer on selected classes\n",
    "    detections = byte_tracker.update_with_detections(detections)\n",
    "    detections = smoother.update_with_detections(detections)\n",
    "\n",
    "    # copy frame before annotate                      \n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    for i, (zone, zone_annotator, box_annotator, trace_annotator, line_zone,\n",
    "            line_zone_annotator,\n",
    "            label_annotator,\n",
    "            line_start,\n",
    "            line_end,\n",
    "            view_transformer,\n",
    "            speed_label,coordinate\n",
    "            ) in  enumerate(zip(zones, zone_annotators, box_annotators,\n",
    "                                trace_annotators,\n",
    "                                line_zones,\n",
    "                                line_zone_annotators,\n",
    "                                label_annotators,\n",
    "                                lines_start,\n",
    "                                lines_end,\n",
    "                                view_transformers,\n",
    "                                speed_labels,\n",
    "                                coordinates\n",
    "                                )\n",
    "                            ):\n",
    "\n",
    "        mask = zone.trigger(detections=detections)\n",
    "        detections_filtered = detections[mask]\n",
    "        points = detections_filtered.get_anchors_coordinates(\n",
    "                anchor=sv.Position.BOTTOM_CENTER)\n",
    "\n",
    "        # plug the view transformer into an existing detection pipeline\n",
    "        \n",
    "        points = view_transformer.transform_points(points=points).astype(int)\n",
    "        \n",
    "        for tracker_id, [_, y] in zip(detections_filtered.tracker_id, points):\n",
    "            coordinate[tracker_id].append(y)\n",
    "\n",
    "        # wait to have enough data\n",
    "        for tracker_id in detections_filtered.tracker_id:\n",
    "                        if len(coordinate[tracker_id]) < fps/2:\n",
    "                            # print(coordinates[tracker_id], \" - id :\", tracker_id, 'len : ', len(coordinates[tracker_id]))\n",
    "                            speed_label.append(f\"#{tracker_id}\")\n",
    "                            \n",
    "                        else:\n",
    "                            try:\n",
    "                                coordinate_start = coordinate[tracker_id][-1]\n",
    "                                coordinate_end = coordinate[tracker_id][0]\n",
    "                                distance = abs(coordinate_start - coordinate_end)\n",
    "                                time = len(coordinate[tracker_id]) / fps\n",
    "                                speed = distance / time * 3.6\n",
    "                                speed_label.append(f\"{int(speed)} km/h\")\n",
    "\n",
    "                            except: \n",
    "\n",
    "                                speed_label.append(f\"#{tracker_id}\")\n",
    "\n",
    "                                pass\n",
    "        # labels = [\n",
    "        # f\"#{tracker_id} \"\n",
    "        # for _,_,_,_,tracker_id in detections_filtered]\n",
    "        # crossed_in, crossed_out= line_zone.trigger(detections=detections_filtered)\n",
    "        # print(crossed_in, crossed_out)\n",
    "        # if line_zone.in_count > 0 or line_zone.out_count > 0:\n",
    "            # print(f\"Zone {i} : {line_zone.in_count} in, {line_zone.out_count} out\")\n",
    "        annotated_frame = sv.draw_line(scene=annotated_frame, start=line_start, end=line_end, color=colors.by_idx(i) )\n",
    "        # annotated_frame = zone_annotator.annotate(scene=annotated_frame, label=f\"Dir. Ouest : {i+random.randint(0,100)}\")\n",
    "        \n",
    "        annotated_frame = zone_annotator.annotate(\n",
    "            scene=annotated_frame,\n",
    "            label=f\"Dir. Ouest : {line_zone.out_count}\") if i==0 else zone_annotator.annotate(\n",
    "                scene=annotated_frame, label=f\"Dir. Est : {line_zone.in_count}\") \n",
    "            \n",
    "        annotated_frame = label_annotator.annotate(scene=annotated_frame,\n",
    "                                                  detections=detections_filtered,\n",
    "                                                  labels=speed_label)\n",
    "        \n",
    "        # annotated_frame=line_zone_annotator.annotate(annotated_frame,line_counter=line_zone )\n",
    "        annotated_frame = box_annotator.annotate(scene=annotated_frame,\n",
    "                                                  detections=detections_filtered,\n",
    "                                                  )\n",
    "        \n",
    "        annotated_frame = trace_annotator.annotate(scene=annotated_frame,detections=detections_filtered )\n",
    "        line_zone.trigger(detections=detections_filtered)\n",
    "        # print(line_zone.in_count)\n",
    "        # print(line_zone.out_count)\n",
    "       \n",
    "\n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 30\n",
      "image : 1280x720\n"
     ]
    }
   ],
   "source": [
    "# for direct show\n",
    "cap = cv2.VideoCapture(VIDEO)  \n",
    "# fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fps=video_info.fps\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"image : {width}x{height}\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # frame=cv2.resize(frame,(1280,720))\n",
    "    show=process_frame(frame,int(fps))\n",
    "   \n",
    "    fps_monitor.tick()\n",
    "    # fps = fps_monitor()\n",
    "    fps = fps_monitor.fps\n",
    "    fps_text = f\"FPS: {fps:.0f}\"\n",
    "    cv2.putText(show, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Counting\", show)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openvino\n",
      "Version: 2024.6.0\n",
      "Summary: OpenVINO(TM) Runtime\n",
      "Home-page: https://docs.openvino.ai/2023.0/index.html\n",
      "Author: Intel(R) Corporation\n",
      "Author-email: openvino@intel.com\n",
      "License: OSI Approved :: Apache Software License\n",
      "Location: c:\\users\\frank.kubler\\anaconda3\\envs\\kt\\lib\\site-packages\n",
      "Requires: openvino-telemetry, numpy, packaging\n",
      "Required-by: openvino-dev\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frank.kubler\\anaconda3\\envs\\kt\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip show openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 30\n",
      "image : 1280x720\n"
     ]
    }
   ],
   "source": [
    "# to save video instead of displaying it\n",
    "side=0\n",
    "output_file = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec vid√©o pour le format MP4\n",
    "\n",
    "\n",
    "out = cv2.VideoWriter(output_file, fourcc, video_info.fps, (video_info.width, video_info.height))\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO)  \n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"FPS: {fps}\")\n",
    "print(f\"image : {width}x{height}\")\n",
    "# Temps de d√©but de l'enregistrement\n",
    "start_time = time.time()\n",
    "# Dur√©e de l'enregistrement en secondes\n",
    "duration = 120\n",
    "\n",
    "while (time.time() - start_time) < duration:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # frame=cv2.resize(frame,(1280,720))\n",
    "    show=process_frame(frame,0,fps)\n",
    "    fps_monitor.tick()\n",
    "    # fps = fps_monitor()\n",
    "    fps = fps_monitor.fps\n",
    "    fps_text = f\"FPS: {fps:.0f}\"\n",
    "    cv2.putText(show, fps_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # cv2.imshow(\"Counting\", show)\n",
    "    out.write(show)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt to 'models/yolo11s.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18.4M/18.4M [00:03<00:00, 5.25MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è models/yolo11s.pt appears to require 'dill', which is not in Ultralytics requirements.\n",
      "AutoInstall will run now for 'dill' but this feature will be removed in the future.\n",
      "Recommend fixes are to train a new model using the latest 'ultralytics' package or to run a command with an official Ultralytics model, i.e. 'yolo predict model=yolov8n.pt'\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['dill'] not found, attempting AutoUpdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/46/d1/e73b6ad76f0b1fb7f23c35c6d95dbc506a9c8804f43dda8cb5b0fa6331fd/dill-0.3.9-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m920.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dill\n",
      "Successfully installed dill-0.3.9\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 1.9s, installed 1 package: ['dill']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "Ultralytics 8.3.1 üöÄ Python-3.11.10 torch-2.1.0+cu121 CPU (12th Gen Intel Core(TM) i5-1235U)\n",
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models/yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2024.1.0-15008-f4afc983258-releases/2024/1...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success ‚úÖ 3.0s, saved as 'models/yolo11s_openvino_model/' (36.4 MB)\n",
      "\n",
      "Export complete (3.4s)\n",
      "Results saved to \u001b[1m/media/storage/OneDrive-4iTEC/Documents/GitHub/traffic_analysis/models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models/yolo11s_openvino_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=models/yolo11s_openvino_model imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models/yolo11s_openvino_model'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# model=YOLO(\"yolov8m.pt\")\n",
    "# model=YOLO(\"traffic_analysis.pt\")\n",
    "model=YOLO(\"models/yolo11s.pt\")\n",
    "model.export(format='openvino')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
